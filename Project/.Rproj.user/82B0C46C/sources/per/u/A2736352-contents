## Preparing the data
# transform data to stationarity
diffed <- diff(ts, differences = 1)

# lagged dataset
lag=365             #ALTERED this from 1 to 365
lag_transform <- function(x, k=lag) {
  
  lagged =  c(rep(NA, k), x[1:(length(x)-k)])
  DF = as.data.frame(cbind(lagged, x))
  colnames(DF) <- c( paste0('x-', k), 'x')
  DF[is.na(DF)] <- 0
  return(DF)
  
}
supervised = lag_transform(diffed, lag)   
head(supervised)
# supervised$`x-365`[367]

# split into train and test sets
N = nrow(supervised)
n = round(N *0.85, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

## Normalize the data
# scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return(list(scaled_train = as.vector(scaled_train), 
              scaled_test = as.vector(scaled_test),
              scaler= c(min =min(x), max = max(x))))
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train[, 2]
x_train = Scaled$scaled_train[, 1]

y_test = Scaled$scaled_test[, 2]
x_test = Scaled$scaled_test[, 1]

# inverse-transform to revert predicted values to og scale
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

## Define the model
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 25                    # can adjust this, in model tuning phase     # have tried 1, 25, & 50

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, 
             batch_input_shape = c(batch_size, 
                                   X_shape2, 
                                   X_shape3), 
             stateful= TRUE)%>%
  layer_dense(units = 1)

# compile the model
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam( lr= 0.02, decay = 1e-6 ),  
  metrics = c('accuracy')
)
summary(model)

# fit the model
Epochs = 50   
for(i in 1:Epochs ){
  model %>% fit(x_train, 
                y_train, 
                epochs=1, 
                batch_size=batch_size, 
                verbose=1, 
                shuffle=FALSE)
  #model %>% reset_states()
}

# Make our predictions
L = length(x_test)
scaler = Scaled$scaler
lstm_forecast = numeric(L)

for(i in 1:L){
  X = x_test[i]
  dim(X) = c(1,1,1)
  yhat = model %>% 
    predict(X, batch_size=batch_size)
  # invert scaling
  yhat = invert_scaling(yhat, scaler,  c(-1, 1))
  # invert differencing
  yhat  = yhat + ts[(n+i)]
  # store
  lstm_forecast[i] <- yhat
}


## Create forecast object to use with our other functions
# prediction on our training set
L = length(x_train)
fitted = numeric(L)

for(i in 1:L){
  X = x_train[i]
  dim(X) = c(1,1,1)
  yhat = model %>% 
    predict(X, batch_size=batch_size) 
  # invert scaling
  yhat = invert_scaling(yhat, scaler,  c(-1, 1))     
  # invert differencing
  yhat  = yhat + ts[(i+1)]
  # store
  fitted[i] <- yhat
}

# deal with the prediction length/lag offset at the beginning of series 
#lstm_forecast <- c(rep(NA, n+1), lstm_forecast)
#fitted <- c(rep(NA, 1), fitted) 

# change predicted values to a time series object
lstm_forecast <- ts(lstm_forecast, start=as.Date("2015-05-30"), frequency =1)         
# input series to a ts object
input_ts <- ts(ts_df$sales, start=as.Date("2013-01-01"), frequency = 1)  

# define forecast object
forecast_list <- list(
  model = NULL,
  method = glue("LSTM (memory: {lag})"),
  mean = lstm_forecast,
  x = input_ts,
  fitted = fitted,
  residuals = as.numeric(input_ts) - as.numeric(c(fitted, rep(NA, length(x_test))))
)
class(forecast_list) <- "forecast"


# Plot it
fc_df <- tibble(date = ts_df$date,
                x = forecast_list$x,
                fitted = c(forecast_list$fitted, rep(NA, length(x_test))),
                forecasted = forecast_list$mean)
# y = sales
ggplot(fc_df, aes(x=date)) +
  geom_line(aes(y=x, color="original series"), size=1) +
  geom_line(aes(y=fitted, color="fitted"), size=1) +
  geom_line(aes(y=forecasted, color="forecast"), size=1) +
  scale_color_manual(name="Model",
                     values=c("original series"="black",
                              "fitted"="turquoise4",
                              "forecast"="red"))
# just the predictions + test
ggplot(data=fc_df[length(x_train+1):N,], aes(x=date)) +
  geom_line(aes(y=x, color="test"), size=1) +
  geom_line(aes(y=forecasted, color="prediction"), size=1) +
  scale_color_manual(name="Model",
                     values = c("test" = "black",
                                "prediction" = "red"))


# Try everything out
train_test_split(ts_df$sales, split_perc = 0.85) #have to run again to get the initial sets back 
#had to temporarily change the length statement in the eval_df assignment - FIX LATER
#i.e.,I added +1 to length(test) to compensate for n-1 fitted values. Could possibly correct with an
#NA for the first value if lag=1 then get rid of the +1, or create a parameter to take care of this.
plot_eval_forecast(ts=forecast_list$x, forecast=forecast_list, 
                   test=test, train=train, og_df.date_col = ts_df$date) 
eval_forecast(forecast_list$x, forecast_list, train=train, test=test)